# RAG - License Intelligence System

**Version:** 0.4\
**Status:** Production Ready

A high-precision legal research tool that answers questions exclusively from market data license agreements using Retrieval-Augmented Generation (RAG). Powered by OpenAI for maximum accuracy.

______________________________________________________________________

## What Is This?

A **grounded legal analysis system** that:

- ‚úÖ Answers questions **only** from uploaded license documents
- ‚úÖ Explicitly refuses to answer when documents are silent
- ‚úÖ Provides exact **citations** (source, document, section, page)
- ‚úÖ Supports multiple data providers (CME, OPRA, CTA/UTP)
- ‚úÖ Uses OpenAI GPT-4.1 for industry-leading accuracy
- ‚ùå **Not** a general chatbot
- ‚ùå **Not** a trained LLM that makes assumptions

### Supported Data Providers

| Provider  | Status     | Documents | Priority |
| --------- | ---------- | --------- | -------- |
| CME Group | ‚úÖ Active  | ~44       | P0       |
| OPRA      | ‚è≥ Planned | TBD       | P1       |
| CTA/UTP   | ‚è≥ Planned | TBD       | P2       |

üìã **[View all data sources ‚Üí](docs/data-sources.md)**

______________________________________________________________________

## Quick Start

### Installation

**Option 1: Docker (Recommended for Production)**

```bash
# 1. Clone repository
git clone <repo-url>
cd licencing-rag

# 2. Configure environment
cp .env.example .env
# Edit .env with your OPENAI_API_KEY and other settings

# 3. Start with Docker Compose
docker-compose up -d

# 4. Verify API is running
curl http://localhost:8000/health

# 5. Install package locally for data ingestion
pip install uv
uv sync
pip install -e .

# 6. Add documents and ingest
mkdir -p data/raw/cme
cp your-documents/*.pdf data/raw/cme/
rag ingest --source cme

# 7. Now the API is ready to query
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer your-api-key" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the fees?"}'
```

**Note:** Data ingestion must be done before the API can answer queries. You need to install the package locally to run `rag ingest`, even when using Docker for the API.

üìñ **[Docker deployment guide ‚Üí](docs/docker-deployment.md)**

**Option 2: Local Development**

```bash
# 1. Clone repository
git clone <repo-url>
cd licencing-rag

# 2. Install dependencies (requires Python 3.13+)
pip install uv  # Fast package manager
uv sync
pip install -e .

# 3. Set OpenAI API key
export OPENAI_API_KEY="sk-..."  # Get from platform.openai.com

# 4. Add documents
mkdir -p data/raw/cme
cp your-documents/*.pdf data/raw/cme/

# 5. Ingest documents (REQUIRED before querying)
rag ingest --source cme
```

### Basic Usage

**‚ö†Ô∏è Important:** You must run `rag ingest` before querying. The system cannot answer questions until documents are indexed.

```bash
# After ingestion, you can:

# Ask questions
rag query "What is a subscriber?"
rag query "What are the redistribution fees?"

# View indexed documents
rag list --source cme
```

**Cost:** ~$0.03 per query (~$90/month for 100 queries/day)

üìñ **[See cost breakdown ‚Üí](docs/cost-estimation.md)**

______________________________________________________________________

## Features

### Core Capabilities

- **TXT, PDF & DOCX Support** - Automatic text extraction with page tracking
- **Multi-Provider** - Organize documents by data source (CME, OPRA, etc.)
- **Hybrid Search** - Combines semantic (vector) and keyword (BM25) search
- **Auto-Linking** - Automatically retrieves definitions when terms appear
- **Accurate Refusals** - Explicitly states when answer is not in documents
- **Rich Citations** - Every answer includes document references

### Advanced Features

- **LLM Reranking** - GPT-4.1 scores chunks for relevance (0-3 scale)
- **Confidence Gating** - Code-enforced refusal when evidence is weak
- **Context Budget** - Enforces ‚â§60k token limit for LLM context
- **Query Normalization** - Removes filler words for better search
- **Debug Mode** - Full pipeline transparency for accuracy verification
- **Audit Logging** - Query tracking for compliance

### Output Formats

- **Console** - Rich formatted output with panels and tables (default)
- **JSON** - Structured data for programmatic access

______________________________________________________________________

## Installation

### Prerequisites

- **Python 3.13+**
- **OpenAI API key** - [Sign up here](https://platform.openai.com/)
- **uv package manager** - [Install guide](https://docs.astral.sh/uv/)

### Step-by-Step Setup

```bash
# 1. Clone and navigate
git clone <repo-url>
cd licencing-rag

# 2. Install dependencies
uv sync
pip install -e .

# 3. Configure API key
export OPENAI_API_KEY="sk-..."

# 4. Verify installation
rag --help
```

### First-Time Setup

```bash
# Delete any old incompatible indexes
make clean-all

# Ingest your first documents
mkdir -p data/raw/cme
# (Copy your PDFs to data/raw/cme/)
rag ingest --source cme

# Test with a query
rag query "What are the fees?"
```

______________________________________________________________________

## Usage

### Document Management

#### Organizing Documents

```
data/raw/
‚îî‚îÄ‚îÄ cme/                    # Provider name
    ‚îú‚îÄ‚îÄ Fees/               # Optional subdirectories
    ‚îÇ   ‚îú‚îÄ‚îÄ january-2025-fee-list.pdf
    ‚îÇ   ‚îî‚îÄ‚îÄ schedule-a.pdf
    ‚îî‚îÄ‚îÄ Agreements/
        ‚îú‚îÄ‚îÄ main-agreement.pdf
        ‚îî‚îÄ‚îÄ subscriber-terms.pdf
```

**Supported formats:** PDF (text-based), DOCX, TXT

‚ùå **Not supported:** Scanned PDFs (OCR not included)

#### Loading Documents

```bash
# Ingest all documents for a provider
rag ingest --source cme

# View indexed documents
rag list --source cme

# View all sources
rag list
```

**What happens during ingestion:**

1. Scans `data/raw/{source}/` recursively for PDF/DOCX files
1. Extracts text with page tracking
1. Chunks documents (500-800 words, section-aware)
1. Generates embeddings via OpenAI (3072 dimensions)
1. Stores in ChromaDB vector database
1. Builds BM25 keyword index

### Querying

#### Basic Queries

```bash
# Simple question
rag query "What is a subscriber?"

# Question about fees
rag query "What are the CME redistribution fees?"

# Multi-part question
rag query "What are the requirements and fees for non-display use?"
```

#### Advanced Options

```bash
# Query specific source
rag query --source cme "What are the fees?"

# Query multiple sources
rag query --source cme --source opra "What is a subscriber?"

# JSON output for automation
rag query --format json "What are the fees?" > result.json

# Search modes
rag query --search-mode vector "..."    # Vector-only (semantic)
rag query --search-mode keyword "..."   # Keyword-only (BM25)
rag query --search-mode hybrid "..."    # Both (default, recommended)

# Debug mode (see pipeline details)
rag query --debug "What are the fees?"
```

#### Understanding Answers

**Console Output:**

```
‚ï≠‚îÄ‚îÄ‚îÄ Answer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ The subscriber fee is $100 per month according to     ‚îÇ
‚îÇ Schedule A Section 3.1.                               ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

‚ï≠‚îÄ‚îÄ‚îÄ Supporting Clauses ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ "Monthly fees for subscriber access shall be $100    ‚îÇ
‚îÇ per individual subscriber..."                         ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Source: cme                                           ‚îÇ
‚îÇ Document: Fees/Schedule-A.pdf                         ‚îÇ
‚îÇ Section: Section 3.1 Pricing                          ‚îÇ
‚îÇ Pages: 5                                              ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

‚ï≠‚îÄ‚îÄ‚îÄ Definitions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ Subscriber: "Any individual authorized to receive     ‚îÇ
‚îÇ market data under a license agreement..."             ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ Source: cme                                           ‚îÇ
‚îÇ Document: Agreements/Main-Agreement.pdf               ‚îÇ
‚îÇ Section: Definitions                                  ‚îÇ
‚îÇ Pages: 2                                              ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

**JSON Output Structure:**

```json
{
  "answer": "The subscriber fee is $100 per month...",
  "supporting_clauses": [
    {
      "text": "Clause text from document...",
      "source": {
        "source": "cme",
        "document": "Fees/Schedule-A.pdf",
        "section": "Section 3.1 Pricing",
        "page_start": 5,
        "page_end": 5
      }
    }
  ],
  "definitions": [
    {
      "term": "Subscriber",
      "definition": "\"Subscriber\" means any individual...",
      "source": { /* ... */ }
    }
  ],
  "citations": [ /* ... */ ],
  "metadata": {
    "sources": ["cme"],
    "chunks_retrieved": 5,
    "search_mode": "hybrid",
    "timestamp": "2026-01-27T10:30:00+00:00"
  }
}
```

### Configuration

#### Environment Variables

| Variable         | Required | Default        | Description         |
| ---------------- | -------- | -------------- | ------------------- |
| `OPENAI_API_KEY` | Yes      | -              | OpenAI API key      |
| `CHROMA_DIR`     | No       | `index/chroma` | Vector DB directory |

#### REST API Configuration

For deploying the REST API, additional environment variables are required:

| Variable                  | Required | Default | Description                            |
| ------------------------- | -------- | ------- | -------------------------------------- |
| `RAG_API_KEY`             | Yes\*    | -       | API key for `/query` and `/sources`    |
| `SLACK_SIGNING_SECRET`    | Yes\*    | -       | Slack app signing secret               |
| `RAG_TEST_MODE`           | No       | `false` | Disable auth for testing (‚ö†Ô∏è dev only) |
| `RAG_RATE_LIMIT`          | No       | `100`   | Max requests per minute per API key    |
| `RAG_CORS_ORIGINS`        | No       | (none)  | Comma-separated allowed origins        |
| `RAG_TRUST_PROXY_HEADERS` | No       | `false` | Trust X-Forwarded-For (behind proxy)   |

\* Required when running the REST API

**Docker Deployment Variables:**

| Variable   | Default | Description                                  |
| ---------- | ------- | -------------------------------------------- |
| `WORKERS`  | `1`     | Uvicorn workers (Compose overrides to 4)     |
| `USER_UID` | `1000`  | Container user UID (match host for volumes)  |
| `USER_GID` | `1000`  | Container group GID (match host for volumes) |

> **Note**: The Dockerfile defaults `WORKERS=1` for development. The docker-compose.yml overrides this to `WORKERS=4` for production workloads. For manual `docker run`, set `WORKERS` based on CPU cores: `(2 * cores) + 1`.

See [`.env.example`](.env.example) for a complete configuration template.

üìñ **[Deployment guide ‚Üí](docs/development/deployment-specs.md)**

#### Advanced Settings

Edit `app/config.py` to customize:

```python
# Models
EMBEDDING_MODEL = "text-embedding-3-large"  # OpenAI embeddings
LLM_MODEL = "gpt-4.1"                       # Answer generation

# Chunking
CHUNK_SIZE = 500          # Target words per chunk
CHUNK_OVERLAP = 100       # Overlap between chunks
MIN_CHUNK_SIZE = 100      # Minimum viable chunk

# Search
TOP_K = 10                # Chunks to retrieve
DEFAULT_SEARCH_MODE = "hybrid"  # vector, keyword, or hybrid

# Budget
MAX_CONTEXT_TOKENS = 60000  # Max tokens for LLM context
```

üìñ **[Complete configuration guide ‚Üí](docs/configuration.md)**

______________________________________________________________________

## How It Works

### Query Pipeline

```
User Question
    ‚Üì
Query Normalization (remove filler words)
    ‚Üì
Embedding (OpenAI text-embedding-3-large)
    ‚Üì
Hybrid Search (Vector + BM25 ‚Üí RRF)
    ‚Üì
LLM Reranking (GPT-4.1, 0-3 relevance scoring)
    ‚Üì
Confidence Gate (refuse if evidence weak)
    ‚Üì
Context Budget (enforce ‚â§60k tokens)
    ‚Üì
Answer Generation (GPT-4.1)
    ‚Üì
Validation (format check)
    ‚Üì
Audit Log (logs/queries.jsonl)
    ‚Üì
Answer + Citations
```

### Key Technologies

| Component      | Technology             | Purpose           |
| -------------- | ---------------------- | ----------------- |
| LLM            | GPT-4.1 (OpenAI)       | Answer generation |
| Embeddings     | text-embedding-3-large | 3072-dim vectors  |
| Vector DB      | ChromaDB               | Semantic search   |
| Keyword Search | BM25                   | Keyword matching  |
| PDF Extract    | PyMuPDF                | Text extraction   |
| DOCX Extract   | python-docx            | DOCX parsing      |

### Why Hybrid Search?

**Vector search** (semantic): Understands meaning, not just keywords\
**Keyword search** (BM25): Finds exact terms, acronyms, numbers\
**Hybrid (RRF)**: Combines both for maximum recall and precision

üìñ **[Learn more about hybrid search ‚Üí](docs/hybrid-search.md)**

______________________________________________________________________

## Workflow Examples

### Initial Setup

**Complete workflow from scratch:**

```bash
# 1. Install system
uv sync && pip install -e .

# 2. Configure API
export OPENAI_API_KEY="sk-..."

# 3. Add documents
mkdir -p data/raw/cme/Fees
cp my-fee-schedule.pdf data/raw/cme/Fees/

# 4. Ingest documents (REQUIRED - builds indexes)
rag ingest --source cme

# 5. Now you can query
rag query "What are the fees?"
```

**Why ingestion is required:**

- Extracts text from PDFs/DOCX
- Chunks documents into searchable segments
- Generates embeddings via OpenAI
- Builds ChromaDB vector index and BM25 keyword index
- Without ingestion, there's nothing to query!

### Regular Use

```bash
# Ask question
rag query "What is a non-display use?"

# Get JSON for automation
rag query --format json "What are subscriber requirements?" | jq .

# Debug pipeline
rag query --debug "What are redistribution fees?"
```

### Adding New Documents

```bash
# 1. Copy files to data/raw/{source}/
cp new-agreement.pdf data/raw/cme/Agreements/

# 2. Re-ingest (updates index)
rag ingest --source cme

# 3. Query new content
rag query "What does the new agreement say about fees?"
```

### Multiple Providers

```bash
# Set up new provider
mkdir -p data/raw/opra
cp opra-docs/*.pdf data/raw/opra/

# Ingest new provider
rag ingest --source opra

# Query specific provider
rag query --source opra "What are OPRA fees?"

# Query all providers
rag query "What are subscriber fees across all providers?"
```

______________________________________________________________________

## Limitations & Risks

### Known Limitations

1. **Text-based PDFs only** - No OCR for scanned documents
1. **English only** - No multilingual support
1. **No real-time updates** - Must re-ingest for document changes
1. **API dependency** - Requires OpenAI API access (internet + billing)
1. **Token limits** - Very long documents may exceed context budget
1. **No legal advice** - System provides information, not counsel

### Accuracy Considerations

- **Source quality matters** - Garbage in, garbage out
- **Context limits** - May miss relevant info in extremely long documents
- **Hallucination risk** - LLM may occasionally misinterpret chunks
- **Always verify** - Cross-check critical answers with source documents

### Operational Risks

- **Cost exposure** - OpenAI API charges per token (monitor usage)
- **Rate limits** - May hit API limits with heavy usage
- **Data privacy** - OpenAI processes your queries (review their terms)
- **Single point of failure** - System depends on OpenAI availability

### Best Practices

1. **Use debug mode** for critical queries (`--debug`)
1. **Verify citations** by checking source documents
1. **Monitor costs** with OpenAI dashboard
1. **Keep documents updated** via regular re-ingestion
1. **Test evaluation set** after major changes

üìä **[View evaluation results ‚Üí](eval/results.json)**

______________________________________________________________________

## Troubleshooting

### Common Issues

#### "OPENAI_API_KEY environment variable is required"

```bash
# Solution: Set your API key
export OPENAI_API_KEY="sk-..."
```

Get a key at [platform.openai.com/api-keys](https://platform.openai.com/api-keys)

#### "No index found" or "Collection not found"

```bash
# Solution: Ingest documents first
rag ingest --source cme
```

#### "Rate limit exceeded"

**Problem:** Too many OpenAI API requests

**Solutions:**

- Wait 60 seconds and retry
- Upgrade API tier at [platform.openai.com/account/rate-limits](https://platform.openai.com/account/rate-limits)
- Reduce `TOP_K` in config to retrieve fewer chunks

#### Empty or poor extraction results

**Problem:** PDF may be scanned (no text layer)

```bash
# Check extracted text
cat data/text/cme/your-document.pdf.txt

# If empty, document is scanned (OCR not supported)
```

#### Embedding model mismatch

**Problem:** Index built with different model

```bash
# Solution: Delete old index and re-ingest
make clean-all
rag ingest --source cme
```

### Debug Mode

See exactly what's happening:

```bash
# Enable verbose logging
rag query --debug "What are the fees?"

# Debug output shows:
# - Query normalization
# - Embedding generation
# - Hybrid search results
# - Reranking scores
# - Confidence gate decision
# - Token usage
# - LLM prompt and response
```

Output appears on stderr and in `logs/debug.jsonl` (machine-parsable).

### Getting Help

1. **Check documentation:** See [Documentation](#documentation) section
1. **Review logs:** `logs/debug.jsonl` and `logs/queries.jsonl`
1. **Open issue:** [GitHub Issues](https://github.com/your-repo/issues)

______________________________________________________________________

## Documentation

### User Guides

- **[Configuration Guide](docs/configuration.md)** - All settings explained
- **[Cost Estimation](docs/cost-estimation.md)** - Pricing and optimization
- **[Data Sources](docs/data-sources.md)** - Provider document tracking
- **[Hybrid Search](docs/hybrid-search.md)** - How search works
- **[RAG Tutorial](docs/rag-tutorial.md)** - Beginner's guide to RAG

### Developer Resources

- **[Developer Guide](docs/development/DEVELOPER_GUIDE.md)** - Architecture and development
- **[Technical Specs](docs/development/rag.specs.md)** - Complete RAG specification
- **[Implementation Plan](docs/development/rag.implementation-plan.md)** - RAG development roadmap
- **[Deployment Specs](docs/development/deployment-specs.md)** - API deployment architecture
- **[Deployment Plan](docs/development/deployment-implementation-plan.md)** - API deployment progress

### Component Guides

- **[Query Normalization](docs/query-normalization.md)** - Filler word removal and query optimization
- **[Reranking](docs/reranking.md)** - LLM-based relevance scoring
- **[Confidence Gating](docs/confidence-gating.md)** - Enforcing accurate refusals
- **[Debug Mode](docs/debug-mode.md)** - Pipeline transparency and troubleshooting
- **[Audit Logging](docs/audit-logging.md)** - Query tracking for compliance
- **[Ingestion](docs/ingestion.md)** - Document processing and indexing

______________________________________________________________________

## Cost Management

### Pricing Overview

**Per Query (typical):** ~$0.03\
**100 queries/day:** ~$90/month\
**500 queries/day:** ~$450/month

### Cost Breakdown

| Operation | Model                  | Cost per Query |
| --------- | ---------------------- | -------------- |
| Embedding | text-embedding-3-large | ~$0.002        |
| Reranking | gpt-4.1                | ~$0.015        |
| Answer    | gpt-4.1                | ~$0.015        |

### Optimization Tips

1. **Use vector-only search** - Skip BM25 for faster queries
1. **Reduce TOP_K** - Retrieve fewer chunks (costs scale linearly)
1. **Monitor with debug** - `--debug` shows token usage
1. **Batch ingestion** - Re-ingest only changed documents
1. **Use caching** - Definitions are cached automatically

üìñ **[Complete cost guide ‚Üí](docs/cost-estimation.md)**

______________________________________________________________________

## Performance

### Benchmarks

**Ingestion:** ~50 documents in ~5 minutes (depends on doc size)\
**Query:** 1-3 seconds typical (depends on chunk count and LLM response time)\
**Accuracy:** 87.5% chunk recall on evaluation set

### Evaluation Results

| Metric           | Score | Target |
| ---------------- | ----- | ------ |
| Chunk Recall     | 87.5% | ‚â•75%   |
| Refusal Accuracy | 100%  | 100%   |
| False Refusal    | 0%    | 0%     |
| False Acceptance | 0%    | 0%     |

üìä **[View full results ‚Üí](eval/results.json)**

______________________________________________________________________

## Development

### For Contributors

If you want to extend or modify the system:

```bash
# Install with dev dependencies
uv sync
pip install -e ".[dev]"

# Run tests
pytest

# Run quality checks
make qa

# Format code
make format
```

üìñ **[Full developer guide ‚Üí](docs/development/DEVELOPER_GUIDE.md)**

### Project Structure

```
licencing-rag/
‚îú‚îÄ‚îÄ app/            # Application code
‚îú‚îÄ‚îÄ data/           # Documents and processing artifacts
‚îú‚îÄ‚îÄ index/          # Search indices (ChromaDB, BM25)
‚îú‚îÄ‚îÄ logs/           # Query and debug logs
‚îú‚îÄ‚îÄ tests/          # Test suite (77% coverage)
‚îú‚îÄ‚îÄ eval/           # Evaluation framework
‚îî‚îÄ‚îÄ docs/           # Documentation
```

______________________________________________________________________

## Support

### Getting Help

- **Documentation:** See [Documentation](#documentation) section
- **Issues:** [Open a GitHub issue](https://github.com/your-repo/issues)
- **Questions:** Check existing issues or create new one

### Reporting Bugs

Include:

1. Command you ran
1. Error message
1. Debug output (`--debug` flag)
1. OpenAI model versions (in `app/config.py`)

______________________________________________________________________

**Last Updated:** January 30, 2026\
**Version:** 0.4 (Production Ready)
